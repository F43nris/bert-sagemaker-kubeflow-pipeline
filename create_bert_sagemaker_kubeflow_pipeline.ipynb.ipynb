{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Kubeflow Pipeline with BERT and Amazon SageMaker\n",
    "\n",
    "In this notebook, we'll be creating a machine learning pipeline using Kubeflow and Amazon SageMaker. Our pipeline will utilize the BERT model for sentiment analysis.\n",
    "\n",
    "## Install Dependencies\n",
    "\n",
    "First, we need to install some dependencies. These include:\n",
    "\n",
    "- **SageMaker Python SDK**: This SDK makes it easy to train and deploy models using Amazon SageMaker.\n",
    "- **Kubeflow Pipelines SDK**: This SDK allows us to work with Kubeflow Pipelines.\n",
    "- **AWS CLI**: This gives us a way to interact with AWS services from the command line.\n",
    "\n",
    "We can install these dependencies using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker==1.72.0\n",
    "!pip install https://storage.googleapis.com/ml-pipeline/release/0.1.29/kfp.tar.gz --upgrade\n",
    "!pip install awscli==1.18.140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment Variables\n",
    "\n",
    "In this section, we'll set up some environment variables that we'll use throughout the notebook. These include:\n",
    "\n",
    "- **Region**: The AWS region that we'll be using for SageMaker and other AWS services.\n",
    "- **Account ID**: Our AWS account ID.\n",
    "- **Bucket**: The name of the S3 bucket that we'll be using for storing data.\n",
    "- **Role**: The IAM role that we'll be using for SageMaker.\n",
    "\n",
    "We can get these values using the boto3 library, which allows us to interact with AWS services using Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Get the AWS region\n",
    "aws_region_as_slist=!curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone | sed 's/\\(.*\\)[a-z]/\\1/'\n",
    "region = aws_region_as_slist.s\n",
    "print('Region: {}'.format(region))\n",
    "\n",
    "# Get the AWS account ID\n",
    "account_id=boto3.client('sts').get_caller_identity().get('Account')\n",
    "print('Account ID: {}'.format(account_id))\n",
    "\n",
    "# Define the S3 bucket name\n",
    "bucket='sagemaker-{}-{}'.format(region, account_id)\n",
    "print('S3 Bucket: {}'.format(bucket))\n",
    "\n",
    "# Get the IAM role for SageMaker\n",
    "iam_roles = boto3.client(\"iam\").list_roles()[\"Roles\"]\n",
    "for iam_role in iam_roles:\n",
    "    if \"SageMakerExecutionRole\" in iam_role[\"RoleName\"]:\n",
    "        role = iam_role[\"Arn\"]\n",
    "        break\n",
    "print(\"Role: {}\".format(role))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Data from Public S3 to Private S3\n",
    "\n",
    "We're going to copy some data from a public S3 bucket to a private S3 bucket. This data will be used later in our pipeline. The data consists of Amazon reviews for digital software, digital video games, and gift cards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_public_path_tsv = \"s3://amazon-reviews-pds/tsv\"\n",
    "s3_private_path_tsv = \"s3://{}/amazon-reviews-pds/tsv\".format(bucket)\n",
    "print(s3_private_path_tsv)\n",
    "\n",
    "# Copy the data from the public to the private S3 bucket\n",
    "!aws s3 cp --recursive $s3_public_path_tsv/ $s3_private_path_tsv/ --exclude \"*\" --include \"amazon_reviews_us_Digital_Software_v1_00.tsv.gz\"\n",
    "!aws s3 cp --recursive $s3_public_path_tsv/ $s3_private_path_tsv/ --exclude \"*\" --include \"amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\"\n",
    "!aws s3 cp --recursive $s3_public_path_tsv/ $s3_private_path_tsv/ --exclude \"*\" --include \"amazon_reviews_us_Gift_Card_v1_00.tsv.gz\"\n",
    "\n",
    "# Define the S3 URI for the raw input data\n",
    "raw_input_data_s3_uri = \"s3://{}/amazon-reviews-pds/tsv/\".format(bucket)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Pipeline\n",
    "\n",
    "In this section, we're going to build our pipeline. We'll be using the Amazon SageMaker Components for Kubeflow Pipelines, which are a set of Kubeflow Pipelines (KFP) components that make it easy to use Amazon SageMaker with Kubeflow Pipelines.\n",
    "\n",
    "The Amazon SageMaker Components for Kubeflow Pipelines include components for:\n",
    "\n",
    "- Processing: Preprocessing data and evaluating models.\n",
    "- Training: Training a model. This includes defining the training script and setting up the training environment.\n",
    "- Model: Creating a model in Amazon SageMaker.\n",
    "- Deploy: Deploying a model to a real-time endpoint.\n",
    "\n",
    "For more information, you can check out the following resources:\n",
    "\n",
    "- [Amazon SageMaker Components for Kubeflow Pipelines on GitHub](https://github.com/kubeflow/pipelines/tree/master/components/aws/sagemaker)\n",
    "- [Using Amazon SageMaker with Kubeflow Pipelines](https://docs.aws.amazon.com/sagemaker/latest/dg/usingamazon-sagemaker-components.html)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this section of the code, we're using the `load_component_from_url` function from the `kfp.components` module to load the Amazon SageMaker components. These components are defined in YAML files hosted on GitHub. Each component corresponds to a specific operation in Amazon SageMaker, such as processing, training, creating a model, or deploying a model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import components\n",
    "from kfp import dsl\n",
    "from kfp.aws import use_aws_secret\n",
    "\n",
    "# Load the components\n",
    "sagemaker_process_op = components.load_component_from_url(\n",
    "    \"https://raw.githubusercontent.com/kubeflow/pipelines/3ebd075212e0a761b982880707ec497c36a99d80/components/aws/sagemaker/process/component.yaml\"\n",
    ")\n",
    "sagemaker_train_op = components.load_component_from_url(\n",
    "    \"https://raw.githubusercontent.com/kubeflow/pipelines/3ebd075212e0a761b982880707ec497c36a99d80/components/aws/sagemaker/train/component.yaml\"\n",
    ")\n",
    "sagemaker_model_op = components.load_component_from_url(\n",
    "    \"https://raw.githubusercontent.com/kubeflow/pipelines/3ebd075212e0a761b982880707ec497c36a99d80/components/aws/sagemaker/model/component.yaml\"\n",
    ")\n",
    "sagemaker_deploy_op = components.load_component_from_url(\n",
    "    \"https://raw.githubusercontent.com/kubeflow/pipelines/3ebd075212e0a761b982880707ec497c36a99d80/components/aws/sagemaker/deploy/component.yaml\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Pre-Processing Code\n",
    "\n",
    "The pre-processing code is responsible for transforming raw data into a format that the model can be trained on. In this case, we're using a script named `preprocess-scikit-text-to-bert-feature-store.py`. We'll upload this script to an S3 bucket so that it can be accessed during the processing stage of the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_code_s3_uri = \"s3://{}/processing_code/preprocess-scikit-text-to-bert-feature-store.py\".format(bucket)\n",
    "print(processing_code_s3_uri)\n",
    "\n",
    "!aws s3 cp ./preprocess-scikit-text-to-bert-feature-store.py $processing_code_s3_uri"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package and Upload Training Code to S3\n",
    "Next, we'll package and upload the training code to S3. The training code is responsible for defining and training the model. We'll package the code into a tarball and then upload it to an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -cvzf sourcedir.tar.gz -C ./code .\n",
    "training_code_s3_uri = \"s3://{}/training_code/sourcedir.tar.gz\".format(bucket)\n",
    "print(training_code_s3_uri)\n",
    "\n",
    "!aws s3 cp sourcedir.tar.gz $training_code_s3_uri"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions\n",
    "We'll define a few helper functions that will be used to configure the inputs and outputs for the processing and training stages of the pipeline.\n",
    "\n",
    "\n",
    "In the `processing_input` function, we're defining the configuration for the input data for the processing stage. This includes the name of the input, the S3 URI where the input data is stored, the local path where the input data will be downloaded to, and the data distribution type.\n",
    "\n",
    "The `processing_output` function is similar, but it's used to define the configuration for the output data from the processing stage.\n",
    "\n",
    "The `training_input` function is used to define the configuration for the input data for the training stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_input(input_name, s3_uri, local_path, s3_data_distribution_type):\n",
    "    return {\n",
    "        \"InputName\": input_name,\n",
    "        \"S3Input\": {\n",
    "            \"LocalPath\": local_path,\n",
    "            \"S3Uri\": s3_uri,\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3DataDistributionType\": s3_data_distribution_type,\n",
    "            \"S3InputMode\": \"File\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "def processing_output(output_name, s3_uri, local_path, s3_upload_mode):\n",
    "    return {\n",
    "        \"OutputName\": output_name,\n",
    "        \"S3Output\": {\"LocalPath\": local_path, \"S3Uri\": s3_uri, \"S3UploadMode\": s3_upload_mode},\n",
    "    }\n",
    "\n",
    "def training_input(input_name, s3_uri, s3_data_distribution_type):\n",
    "    return {\n",
    "        \"ChannelName\": input_name,\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3Uri\": s3_uri,\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"S3DataDistributionType\": s3_data_distribution_type,\n",
    "            }\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Pipeline\n",
    "\n",
    "The pipeline is defined using the `@dsl.pipeline` decorator, which takes in the name and description of the pipeline. The pipeline function `bert_pipeline` takes in the role, bucket, region, and raw_input_data_s3_uri as arguments.\n",
    "\n",
    "The pipeline consists of three main stages: feature engineering, training, and deployment.\n",
    "\n",
    "In the feature engineering stage, we're using the sagemaker_process_op to run the pre-processing script that we uploaded to S3 earlier. The output of this stage is the processed data, which is stored in S3.\n",
    "\n",
    "In the training stage, we're using the sagemaker_train_op to train the model. The training script and the processed data from the previous stage are used as inputs. The output of this stage is the trained model, which is stored in S3.\n",
    "\n",
    "In the deployment stage, we're using the sagemaker_model_op to create a model from the trained model artifact, and then the sagemaker_deploy_op to deploy the model to a SageMaker endpoint.\n",
    "\n",
    "Each stage in the pipeline is connected using the .after() method, which ensures that the stages are executed in the correct order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "    name=\"BERT Pipeline\",  # Name of the pipeline\n",
    "    description=\"BERT Pipeline\",  # Description of the pipeline\n",
    ")\n",
    "def bert_pipeline(role=role, bucket=bucket, region=region, raw_input_data_s3_uri=raw_input_data_s3_uri):\n",
    "    # Import necessary libraries\n",
    "    import time\n",
    "    import json\n",
    "\n",
    "    # Define the pipeline name with a unique timestamp\n",
    "    pipeline_name = \"kubeflow-pipeline-sagemaker-{}\".format(int(time.time()))\n",
    "\n",
    "    # Set network isolation to False\n",
    "    network_isolation = False\n",
    "\n",
    "    ########################\n",
    "    # FEATURE ENGINEERING\n",
    "    ########################\n",
    "\n",
    "    # Define parameters for feature engineering\n",
    "    max_seq_length = 64\n",
    "    train_split_percentage = 0.90\n",
    "    validation_split_percentage = 0.05\n",
    "    test_split_percentage = 0.05\n",
    "    balance_dataset = True\n",
    "\n",
    "    # Define S3 URIs for the processed data\n",
    "    processed_train_data_s3_uri = \"s3://{}/{}/processing/output/bert-train\".format(bucket, pipeline_name)\n",
    "    processed_validation_data_s3_uri = \"s3://{}/{}/processing/output/bert-validation\".format(bucket, pipeline_name)\n",
    "    processed_test_data_s3_uri = \"s3://{}/{}/processing/output/bert-test\".format(bucket, pipeline_name)\n",
    "\n",
    "    # Define instance type and count for processing\n",
    "    processing_instance_type = \"ml.c5.2xlarge\"\n",
    "    processing_instance_count = 2\n",
    "\n",
    "    # Define feature store prefix and group name\n",
    "    timestamp = int(time.time())\n",
    "    feature_store_offline_prefix = \"reviews-feature-store-\" + str(timestamp)\n",
    "    feature_group_name = \"reviews-feature-group-\" + str(timestamp)\n",
    "\n",
    "    # Define the processing image\n",
    "    processing_image = \"683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3\"\n",
    "\n",
    "    # Define the processing operation\n",
    "   \n",
    "    process = sagemaker_process_op(\n",
    "        role=role,\n",
    "        region=region,\n",
    "        image=processing_image,\n",
    "        network_isolation=network_isolation,\n",
    "        instance_type=processing_instance_type,\n",
    "        instance_count=processing_instance_count,\n",
    "        container_arguments=[\n",
    "            \"--train-split-percentage\",\n",
    "            str(train_split_percentage),\n",
    "            \"--validation-split-percentage\",\n",
    "            str(validation_split_percentage),\n",
    "            \"--test-split-percentage\",\n",
    "            str(test_split_percentage),\n",
    "            \"--max-seq-length\",\n",
    "            str(max_seq_length),\n",
    "            \"--balance-dataset\",\n",
    "            str(balance_dataset),\n",
    "            \"--feature-store-offline-prefix\",\n",
    "            str(feature_store_offline_prefix),\n",
    "            \"--feature-group-name\",\n",
    "            str(feature_group_name),\n",
    "        ],\n",
    "        environment={\"AWS_DEFAULT_REGION\": \"eu-central-1\"},  # hard-coding to avoid serialization issue\n",
    "        container_entrypoint=[\n",
    "            \"python3\",\n",
    "            \"/opt/ml/processing/input/code/preprocess-scikit-text-to-bert-feature-store.py\",\n",
    "        ],\n",
    "        input_config=[\n",
    "            processing_input(\n",
    "                input_name=\"raw-input-data\",\n",
    "                s3_uri=\"{}\".format(raw_input_data_s3_uri),\n",
    "                local_path=\"/opt/ml/processing/input/data/\",\n",
    "                s3_data_distribution_type=\"ShardedByS3Key\",\n",
    "            ),\n",
    "            processing_input(\n",
    "                input_name=\"code\",\n",
    "                s3_uri=\"{}\".format(processing_code_s3_uri),\n",
    "                local_path=\"/opt/ml/processing/input/code\",\n",
    "                s3_data_distribution_type=\"FullyReplicated\",\n",
    "            ),\n",
    "        ],\n",
    "        output_config=[\n",
    "            processing_output(\n",
    "                output_name=\"bert-train\",\n",
    "                s3_uri=\"{}\".format(processed_train_data_s3_uri),\n",
    "                local_path=\"/opt/ml/processing/output/bert/train\",\n",
    "                s3_upload_mode=\"EndOfJob\",\n",
    "            ),\n",
    "            processing_output(\n",
    "                output_name=\"bert-validation\",\n",
    "                s3_uri=\"{}\".format(processed_validation_data_s3_uri),\n",
    "                local_path=\"/opt/ml/processing/output/bert/validation\",\n",
    "                s3_upload_mode=\"EndOfJob\",\n",
    "            ),\n",
    "            processing_output(\n",
    "                output_name=\"bert-test\",\n",
    "                s3_uri=\"{}\".format(processed_test_data_s3_uri),\n",
    "                local_path=\"/opt/ml/processing/output/bert/test\",\n",
    "                s3_upload_mode=\"EndOfJob\",\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    ########################\n",
    "    # TRAIN\n",
    "    ########################\n",
    "\n",
    "    # Define the training channels\n",
    "    train_channels = [\n",
    "        training_input(\n",
    "            input_name=\"train\", s3_uri=processed_train_data_s3_uri, s3_data_distribution_type=\"ShardedByS3Key\"\n",
    "        ),\n",
    "        training_input(\n",
    "            input_name=\"validation\",\n",
    "            s3_uri=processed_validation_data_s3_uri,\n",
    "            s3_data_distribution_type=\"ShardedByS3Key\",\n",
    "        ),\n",
    "        training_input(\n",
    "            input_name=\"test\", s3_uri=processed_test_data_s3_uri, s3_data_distribution_type=\"ShardedByS3Key\"\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Define hyperparameters for training\n",
    "    epochs = 1\n",
    "    learning_rate = 0.00001\n",
    "    epsilon = 0.00000001\n",
    "    train_batch_size = 128\n",
    "    validation_batch_size = 128\n",
    "    test_batch_size = 128\n",
    "    train_steps_per_epoch = 100\n",
    "    validation_steps = 100\n",
    "    test_steps = 100\n",
    "    train_volume_size = 1024\n",
    "    use_xla = True\n",
    "    use_amp = True\n",
    "    freeze_bert_layer = False\n",
    "    enable_sagemaker_debugger = False\n",
    "    enable_checkpointing = False\n",
    "    enable_tensorboard = False\n",
    "    input_mode = \"File\"\n",
    "    run_validation = True\n",
    "    run_test = True\n",
    "    run_sample_predictions = True\n",
    "\n",
    "    train_instance_type = \"ml.c5.9xlarge\"\n",
    "    train_instance_count = 1\n",
    "\n",
    "    train_output_location = \"s3://{}/{}/output\".format(bucket, pipeline_name)\n",
    "\n",
    "    hyperparameters = {\n",
    "        \"epochs\": \"{}\".format(epochs),\n",
    "        \"learning_rate\": \"{}\".format(learning_rate),\n",
    "        \"epsilon\": \"{}\".format(epsilon),\n",
    "        \"train_batch_size\": \"{}\".format(train_batch_size),\n",
    "        \"validation_batch_size\": \"{}\".format(validation_batch_size),\n",
    "        \"test_batch_size\": \"{}\".format(test_batch_size),\n",
    "        \"train_steps_per_epoch\": \"{}\".format(train_steps_per_epoch),\n",
    "        \"validation_steps\": \"{}\".format(validation_steps),\n",
    "        \"test_steps\": \"{}\".format(test_steps),\n",
    "        \"use_xla\": \"{}\".format(use_xla),\n",
    "        \"use_amp\": \"{}\".format(use_amp),\n",
    "        \"max_seq_length\": \"{}\".format(max_seq_length),\n",
    "        \"freeze_bert_layer\": \"{}\".format(freeze_bert_layer),\n",
    "        \"enable_sagemaker_debugger\": \"{}\".format(enable_sagemaker_debugger),\n",
    "        \"enable_checkpointing\": \"{}\".format(enable_checkpointing),\n",
    "        \"enable_tensorboard\": \"{}\".format(enable_tensorboard),\n",
    "        \"run_validation\": \"{}\".format(run_validation),\n",
    "        \"run_test\": \"{}\".format(run_test),\n",
    "        \"run_sample_predictions\": \"{}\".format(run_sample_predictions),\n",
    "        \"model_dir\": \"{}\".format(train_output_location),\n",
    "        \"sagemaker_program\": \"tf_bert_reviews.py\",\n",
    "        \"sagemaker_region\": \"{}\".format(region),\n",
    "        \"sagemaker_submit_directory\": training_code_s3_uri,\n",
    "    }\n",
    "    hyperparameters_json = json.dumps(hyperparameters)\n",
    "\n",
    "    # metric_definitions='{\"val_acc\": \"val_accuracy: ([0-9\\\\\\\\.]+)\"}',\n",
    "    metrics_definitions = [\n",
    "        {\"Name\": \"train:loss\", \"Regex\": \"loss: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"train:accuracy\", \"Regex\": \"accuracy: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"validation:loss\", \"Regex\": \"val_loss: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"validation:accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"},\n",
    "    ]\n",
    "    metrics_definitions_json = json.dumps(metrics_definitions)\n",
    "    print(metrics_definitions_json)\n",
    "\n",
    "    # .after(process) is explicitly appended below\n",
    "    train_image = \"763104351884.dkr.ecr.{}.amazonaws.com/tensorflow-training:2.3.1-cpu-py37-ubuntu18.04\".format(region)\n",
    "    training = sagemaker_train_op(\n",
    "        region=region,\n",
    "        image=train_image,\n",
    "        network_isolation=network_isolation,\n",
    "        instance_type=train_instance_type,\n",
    "        instance_count=train_instance_count,\n",
    "        hyperparameters=hyperparameters_json,\n",
    "        training_input_mode=input_mode,\n",
    "        channels=train_channels,\n",
    "        model_artifact_path=train_output_location,\n",
    "        # metric_definitions=metrics_definitions_json,\n",
    "        # TODO:  Add rules\n",
    "        role=role,\n",
    "    ).after(process)\n",
    "    ########################\n",
    "    # DEPLOY\n",
    "    ########################\n",
    "\n",
    "    # Define the serving image\n",
    "    serve_image = \"763104351884.dkr.ecr.{}.amazonaws.com/tensorflow-inference:2.3.1-cpu\".format(region)\n",
    "\n",
    "    # Define the model creation operation\n",
    "    create_model = sagemaker_model_op(\n",
    "        region=region,\n",
    "        model_name=training.outputs[\"job_name\"],\n",
    "        image=serve_image,\n",
    "        network_isolation=network_isolation,\n",
    "        model_artifact_url=training.outputs[\"model_artifact_url\"],\n",
    "        role=role,\n",
    "    )\n",
    "\n",
    "    # Define the deployment operation\n",
    "    deploy_instance_type = \"ml.c5.9xlarge\"\n",
    "    deploy_instance_count = 1\n",
    "    # .after(create_model) is implied because we depend on create_model.outputs\n",
    "    deploy_model = sagemaker_deploy_op(\n",
    "        region=region,\n",
    "        variant_name_1=\"AllTraffic\",\n",
    "        model_name_1=create_model.output,\n",
    "        instance_type_1=deploy_instance_type,\n",
    "        initial_instance_count_1=deploy_instance_count,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Kubeflow Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Kubeflow pipeline\n",
    "kfp.compiler.Compiler().compile(bert_pipeline, \"bert-pipeline.zip\")\n",
    "\n",
    "# List the details of the compiled pipeline file\n",
    "!ls -al ./bert-pipeline.zip\n",
    "\n",
    "# Unzip the compiled pipeline file\n",
    "!unzip -o ./bert-pipeline.zip\n",
    "\n",
    "# Display the contents of the pipeline.yaml file using pygmentize for syntax highlighting\n",
    "!pygmentize pipeline.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Pipeline on Kubernetes Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the pipeline on the Kubernetes cluster\n",
    "client = kfp.Client()\n",
    "\n",
    "# Create an experiment in Kubeflow\n",
    "experiment = client.create_experiment(name=\"kubeflow\")\n",
    "\n",
    "# Run the pipeline within the context of the experiment\n",
    "my_run = client.run_pipeline(experiment.id, \"bert-pipeline\", \"bert-pipeline.zip\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above training job may take 5-10 minutes. Please be patient.\n",
    "In the meantime, open the SageMaker Console to monitor the progress of your training job.\n",
    "\n",
    "First, we need to get the endpoint name of our newly-deployed SageMaker Prediction Endpoint.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code first compiles the defined Kubeflow pipeline into a zip file, then it unzips the file and displays the contents of the pipeline.yaml file. After that, it creates a new experiment in Kubeflow and runs the pipeline within the context of the experiment.\n",
    "\n",
    "Once the pipeline is run, it waits for the training job to complete. After the training job is completed, it retrieves the endpoint name of the newly deployed SageMaker prediction endpoint. Then, it uses this endpoint to make predictions on some sample inputs and prints the predicted classes for each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import boto3 library for AWS operations\n",
    "import boto3\n",
    "\n",
    "# Create a SageMaker runtime client\n",
    "sm_runtime = boto3.Session(region_name=region).client(\"sagemaker-runtime\")\n",
    "\n",
    "# Replace this with the actual endpoint name from the Kubeflow pipeline logs\n",
    "endpoint_name = \"<COPY-AND-PASTE-FROM-KUBEFLOW-PIPELINE-LOGS>\"\n",
    "\n",
    "# Define the inputs for prediction\n",
    "inputs = [{\"features\": [\"This is great!\"]}, {\"features\": [\"This is bad.\"]}]\n",
    "\n",
    "# Invoke the SageMaker endpoint for prediction\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/jsonlines\",\n",
    "    Accept=\"application/jsonlines\",\n",
    "    Body=json.dumps(inputs).encode(\"utf-8\"),\n",
    ")\n",
    "\n",
    "# Print the response from the prediction\n",
    "print(\"response: {}\".format(response))\n",
    "\n",
    "# Parse the predicted classes from the response\n",
    "predicted_classes_str = response[\"Body\"].read().decode()\n",
    "predicted_classes_json = json.loads(predicted_classes_str)\n",
    "\n",
    "predicted_classes = predicted_classes_json.splitlines()\n",
    "print(\"predicted_classes: {}\".format(predicted_classes))\n",
    "\n",
    "# Print the predicted class for each input\n",
    "for predicted_class_json, input_data in zip(predicted_classes, inputs):\n",
    "    predicted_class = json.loads(predicted_class_json)[\"predicted_label\"]\n",
    "    print('Predicted star_rating: {} for review_body \"{}\"'.format(predicted_class, input_data[\"features\"][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a68e2ec38d346fc0a81cce9ab4074dbd84a3a16f4775ed0b6a963af8256663df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
