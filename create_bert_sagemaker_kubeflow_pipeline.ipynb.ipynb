{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Kubeflow Pipeline with BERT and Amazon SageMaker\n",
    "\n",
    "In this notebook, we'll be creating a machine learning pipeline using Kubeflow and Amazon SageMaker. Our pipeline will utilize the BERT model for sentiment analysis.\n",
    "\n",
    "## Install Dependencies\n",
    "\n",
    "First, we need to install some dependencies. These include:\n",
    "\n",
    "- **SageMaker Python SDK**: This SDK makes it easy to train and deploy models using Amazon SageMaker.\n",
    "- **Kubeflow Pipelines SDK**: This SDK allows us to work with Kubeflow Pipelines.\n",
    "- **AWS CLI**: This gives us a way to interact with AWS services from the command line.\n",
    "\n",
    "We can install these dependencies using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker==1.72.0\n",
      "  Downloading sagemaker-1.72.0.tar.gz (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.3/297.3 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting boto3>=1.14.12\n",
      "  Downloading boto3-1.26.165-py3-none-any.whl (135 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.9/135.9 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.9.0\n",
      "  Downloading numpy-1.25.0-cp39-cp39-macosx_10_9_x86_64.whl (20.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf>=3.1\n",
      "  Downloading protobuf-4.23.3-cp37-abi3-macosx_10_9_universal2.whl (400 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.3/400.3 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=0.19.0\n",
      "  Downloading scipy-1.11.1-cp39-cp39-macosx_10_9_x86_64.whl (37.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.2/37.2 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf3-to-dict>=0.1.5\n",
      "  Downloading protobuf3-to-dict-0.1.5.tar.gz (3.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting smdebug-rulesconfig==0.1.4\n",
      "  Downloading smdebug_rulesconfig-0.1.4-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in ./venv/lib/python3.9/site-packages (from sagemaker==1.72.0) (6.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.9/site-packages (from sagemaker==1.72.0) (23.1)\n",
      "Collecting jmespath<2.0.0,>=0.7.1\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Collecting s3transfer<0.7.0,>=0.6.0\n",
      "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting botocore<1.30.0,>=1.29.165\n",
      "  Downloading botocore-1.29.165-py3-none-any.whl (11.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in ./venv/lib/python3.9/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.15.0)\n",
      "Requirement already satisfied: six in ./venv/lib/python3.9/site-packages (from protobuf3-to-dict>=0.1.5->sagemaker==1.72.0) (1.16.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in ./venv/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.165->boto3>=1.14.12->sagemaker==1.72.0) (1.26.15)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./venv/lib/python3.9/site-packages (from botocore<1.30.0,>=1.29.165->boto3>=1.14.12->sagemaker==1.72.0) (2.8.2)\n",
      "Using legacy 'setup.py install' for sagemaker, since package 'wheel' is not installed.\n",
      "Using legacy 'setup.py install' for protobuf3-to-dict, since package 'wheel' is not installed.\n",
      "Installing collected packages: smdebug-rulesconfig, protobuf, numpy, jmespath, scipy, protobuf3-to-dict, botocore, s3transfer, boto3, sagemaker\n",
      "  Running setup.py install for protobuf3-to-dict ... \u001b[?25ldone\n",
      "\u001b[?25h  Running setup.py install for sagemaker ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed boto3-1.26.165 botocore-1.29.165 jmespath-1.0.1 numpy-1.25.0 protobuf-4.23.3 protobuf3-to-dict-0.1.5 s3transfer-0.6.1 sagemaker-1.72.0 scipy-1.11.1 smdebug-rulesconfig-0.1.4\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/Users/ADAM-Production/StudioProjects/gpt_ebook_finetuning/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting https://storage.googleapis.com/ml-pipeline/release/0.1.29/kfp.tar.gz\n",
      "  Downloading https://storage.googleapis.com/ml-pipeline/release/0.1.29/kfp.tar.gz (88 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 KB\u001b[0m \u001b[31m622.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting urllib3<1.25,>=1.15\n",
      "  Downloading urllib3-1.24.3-py2.py3-none-any.whl (118 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.8/118.8 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10 in ./venv/lib/python3.9/site-packages (from kfp==0.1.29) (1.16.0)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.9/site-packages (from kfp==0.1.29) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil in ./venv/lib/python3.9/site-packages (from kfp==0.1.29) (2.8.2)\n",
      "Collecting PyYAML\n",
      "  Using cached PyYAML-6.0-cp39-cp39-macosx_10_9_x86_64.whl (197 kB)\n",
      "Collecting google-cloud-storage>=1.13.0\n",
      "  Downloading google_cloud_storage-2.10.0-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting kubernetes<=9.0.0,>=8.0.0\n",
      "  Downloading kubernetes-9.0.0-py2.py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting PyJWT>=1.6.4\n",
      "  Downloading PyJWT-2.7.0-py3-none-any.whl (22 kB)\n",
      "Collecting cryptography>=2.4.2\n",
      "  Downloading cryptography-41.0.1-cp37-abi3-macosx_10_12_x86_64.whl (2.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth>=1.6.1\n",
      "  Downloading google_auth-2.21.0-py2.py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.1/182.1 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests_toolbelt>=0.8.0\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 KB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cloudpickle\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting kfp-server-api<=0.1.25,>=0.1.18\n",
      "  Downloading kfp-server-api-0.1.18.3.tar.gz (33 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting argo-models==2.2.1a\n",
      "  Downloading argo-models-2.2.1a0.tar.gz (28 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jsonschema>=3.0.1\n",
      "  Downloading jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.4/90.4 KB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tabulate==0.8.3\n",
      "  Downloading tabulate-0.8.3.tar.gz (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 KB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting click==7.0\n",
      "  Downloading Click-7.0-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting Deprecated\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting cffi>=1.12\n",
      "  Using cached cffi-1.15.1-cp39-cp39-macosx_10_9_x86_64.whl (179 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 KB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-resumable-media>=2.3.2\n",
      "  Downloading google_resumable_media-2.5.0-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.7/77.7 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-cloud-core<3.0dev,>=2.3.0\n",
      "  Using cached google_cloud_core-2.3.2-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in ./venv/lib/python3.9/site-packages (from google-cloud-storage>=1.13.0->kfp==0.1.29) (2.28.2)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
      "  Downloading google_api_core-2.11.1-py3-none-any.whl (120 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.5/120.5 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0\n",
      "  Downloading pyrsistent-0.19.3-cp39-cp39-macosx_10_9_universal2.whl (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.5/82.5 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.4.0 in ./venv/lib/python3.9/site-packages (from jsonschema>=3.0.1->kfp==0.1.29) (23.1.0)\n",
      "Collecting requests-oauthlib\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in ./venv/lib/python3.9/site-packages (from kubernetes<=9.0.0,>=8.0.0->kfp==0.1.29) (58.1.0)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0\n",
      "  Downloading websocket_client-1.6.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wrapt<2,>=1.10\n",
      "  Downloading wrapt-1.15.0-cp39-cp39-macosx_10_9_x86_64.whl (35 kB)\n",
      "Collecting pycparser\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in ./venv/lib/python3.9/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage>=1.13.0->kfp==0.1.29) (4.23.3)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2\n",
      "  Downloading googleapis_common_protos-1.59.1-py2.py3-none-any.whl (224 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-crc32c<2.0dev,>=1.0\n",
      "  Using cached google_crc32c-1.5.0-cp39-cp39-macosx_10_9_x86_64.whl (30 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 KB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=1.13.0->kfp==0.1.29) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.9/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage>=1.13.0->kfp==0.1.29) (3.1.0)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 KB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing legacy 'setup.py install' for kfp, since package 'wheel' is not installed.\n",
      "Using legacy 'setup.py install' for argo-models, since package 'wheel' is not installed.\n",
      "Using legacy 'setup.py install' for tabulate, since package 'wheel' is not installed.\n",
      "Using legacy 'setup.py install' for kfp-server-api, since package 'wheel' is not installed.\n",
      "Installing collected packages: tabulate, wrapt, websocket-client, urllib3, PyYAML, pyrsistent, PyJWT, pycparser, pyasn1, oauthlib, googleapis-common-protos, google-crc32c, cloudpickle, click, cachetools, rsa, pyasn1-modules, kfp-server-api, jsonschema, google-resumable-media, Deprecated, cffi, requests_toolbelt, requests-oauthlib, google-auth, cryptography, kubernetes, google-api-core, google-cloud-core, argo-models, google-cloud-storage, kfp\n",
      "  Running setup.py install for tabulate ... \u001b[?25ldone\n",
      "\u001b[?25h  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.15\n",
      "    Uninstalling urllib3-1.26.15:\n",
      "      Successfully uninstalled urllib3-1.26.15\n",
      "  Running setup.py install for kfp-server-api ... \u001b[?25ldone\n",
      "\u001b[?25h  Running setup.py install for argo-models ... \u001b[?25ldone\n",
      "\u001b[?25h  Running setup.py install for kfp ... \u001b[?25ldone\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "selenium 4.9.0 requires urllib3[socks]~=1.26, but you have urllib3 1.24.3 which is incompatible.\n",
      "botocore 1.29.165 requires urllib3<1.27,>=1.25.4, but you have urllib3 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Deprecated-1.2.14 PyJWT-2.7.0 PyYAML-6.0 argo-models-2.2.1a0 cachetools-5.3.1 cffi-1.15.1 click-7.0 cloudpickle-2.2.1 cryptography-41.0.1 google-api-core-2.11.1 google-auth-2.21.0 google-cloud-core-2.3.2 google-cloud-storage-2.10.0 google-crc32c-1.5.0 google-resumable-media-2.5.0 googleapis-common-protos-1.59.1 jsonschema-4.17.3 kfp-0.1.29 kfp-server-api-0.1.18.3 kubernetes-9.0.0 oauthlib-3.2.2 pyasn1-0.5.0 pyasn1-modules-0.3.0 pycparser-2.21 pyrsistent-0.19.3 requests-oauthlib-1.3.1 requests_toolbelt-1.0.0 rsa-4.9 tabulate-0.8.3 urllib3-1.24.3 websocket-client-1.6.1 wrapt-1.15.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/Users/ADAM-Production/StudioProjects/gpt_ebook_finetuning/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting awscli==1.18.140\n",
      "  Downloading awscli-1.18.140-py2.py3-none-any.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting colorama<0.4.4,>=0.2.5\n",
      "  Downloading colorama-0.4.3-py2.py3-none-any.whl (15 kB)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.6/547.6 KB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting botocore==1.17.63\n",
      "  Downloading botocore-1.17.63-py2.py3-none-any.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting PyYAML<5.4,>=3.10\n",
      "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.4/269.4 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting rsa<=4.5.0,>=3.1.2\n",
      "  Downloading rsa-4.5-py2.py3-none-any.whl (36 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./venv/lib/python3.9/site-packages (from botocore==1.17.63->awscli==1.18.140) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20 in ./venv/lib/python3.9/site-packages (from botocore==1.17.63->awscli==1.18.140) (1.24.3)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in ./venv/lib/python3.9/site-packages (from rsa<=4.5.0,>=3.1.2->awscli==1.18.140) (0.5.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.17.63->awscli==1.18.140) (1.16.0)\n",
      "Using legacy 'setup.py install' for PyYAML, since package 'wheel' is not installed.\n",
      "Installing collected packages: rsa, PyYAML, jmespath, docutils, colorama, botocore, s3transfer, awscli\n",
      "  Attempting uninstall: rsa\n",
      "    Found existing installation: rsa 4.9\n",
      "    Uninstalling rsa-4.9:\n",
      "      Successfully uninstalled rsa-4.9\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Running setup.py install for PyYAML ... \u001b[?25ldone\n",
      "\u001b[?25h  Attempting uninstall: jmespath\n",
      "    Found existing installation: jmespath 1.0.1\n",
      "    Uninstalling jmespath-1.0.1:\n",
      "      Successfully uninstalled jmespath-1.0.1\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.29.165\n",
      "    Uninstalling botocore-1.29.165:\n",
      "      Successfully uninstalled botocore-1.29.165\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.6.1\n",
      "    Uninstalling s3transfer-0.6.1:\n",
      "      Successfully uninstalled s3transfer-0.6.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "boto3 1.26.165 requires botocore<1.30.0,>=1.29.165, but you have botocore 1.17.63 which is incompatible.\n",
      "boto3 1.26.165 requires s3transfer<0.7.0,>=0.6.0, but you have s3transfer 0.3.7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyYAML-5.3.1 awscli-1.18.140 botocore-1.17.63 colorama-0.4.3 docutils-0.15.2 jmespath-0.10.0 rsa-4.5 s3transfer-0.3.7\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/Users/ADAM-Production/StudioProjects/gpt_ebook_finetuning/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker==1.72.0\n",
    "!pip install https://storage.googleapis.com/ml-pipeline/release/0.1.29/kfp.tar.gz --upgrade\n",
    "!pip install awscli==1.18.140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment Variables\n",
    "\n",
    "In this section, we'll set up some environment variables that we'll use throughout the notebook. These include:\n",
    "\n",
    "- **Region**: The AWS region that we'll be using for SageMaker and other AWS services.\n",
    "- **Account ID**: Our AWS account ID.\n",
    "- **Bucket**: The name of the S3 bucket that we'll be using for storing data.\n",
    "- **Role**: The IAM role that we'll be using for SageMaker.\n",
    "\n",
    "We can get these values using the boto3 library, which allows us to interact with AWS services using Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Get the AWS region\n",
    "aws_region_as_slist=!curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone | sed 's/\\(.*\\)[a-z]/\\1/'\n",
    "region = aws_region_as_slist.s\n",
    "print('Region: {}'.format(region))\n",
    "\n",
    "# Get the AWS account ID\n",
    "account_id=boto3.client('sts').get_caller_identity().get('Account')\n",
    "print('Account ID: {}'.format(account_id))\n",
    "\n",
    "# Define the S3 bucket name\n",
    "bucket='sagemaker-{}-{}'.format(region, account_id)\n",
    "print('S3 Bucket: {}'.format(bucket))\n",
    "\n",
    "# Get the IAM role for SageMaker\n",
    "iam_roles = boto3.client(\"iam\").list_roles()[\"Roles\"]\n",
    "for iam_role in iam_roles:\n",
    "    if \"SageMakerExecutionRole\" in iam_role[\"RoleName\"]:\n",
    "        role = iam_role[\"Arn\"]\n",
    "        break\n",
    "print(\"Role: {}\".format(role))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Data from Public S3 to Private S3\n",
    "\n",
    "We're going to copy some data from a public S3 bucket to a private S3 bucket. This data will be used later in our pipeline. The data consists of Amazon reviews for digital software, digital video games, and gift cards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bucket' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m s3_public_path_tsv \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39ms3://amazon-reviews-pds/tsv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m s3_private_path_tsv \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39ms3://\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/amazon-reviews-pds/tsv\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(bucket)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(s3_private_path_tsv)\n\u001b[1;32m      5\u001b[0m \u001b[39m# Copy the data from the public to the private S3 bucket\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bucket' is not defined"
     ]
    }
   ],
   "source": [
    "s3_public_path_tsv = \"s3://amazon-reviews-pds/tsv\"\n",
    "s3_private_path_tsv = \"s3://{}/amazon-reviews-pds/tsv\".format(bucket)\n",
    "print(s3_private_path_tsv)\n",
    "\n",
    "# Copy the data from the public to the private S3 bucket\n",
    "!aws s3 cp --recursive $s3_public_path_tsv/ $s3_private_path_tsv/ --exclude \"*\" --include \"amazon_reviews_us_Digital_Software_v1_00.tsv.gz\"\n",
    "!aws s3 cp --recursive $s3_public_path_tsv/ $s3_private_path_tsv/ --exclude \"*\" --include \"amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\"\n",
    "!aws s3 cp --recursive $s3_public_path_tsv/ $s3_private_path_tsv/ --exclude \"*\" --include \"amazon_reviews_us_Gift_Card_v1_00.tsv.gz\"\n",
    "\n",
    "# Define the S3 URI for the raw input data\n",
    "raw_input_data_s3_uri = \"s3://{}/amazon-reviews-pds/tsv/\".format(bucket)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Pipeline\n",
    "\n",
    "In this section, we're going to build our pipeline. We'll be using the Amazon SageMaker Components for Kubeflow Pipelines, which are a set of Kubeflow Pipelines (KFP) components that make it easy to use Amazon SageMaker with Kubeflow Pipelines.\n",
    "\n",
    "The Amazon SageMaker Components for Kubeflow Pipelines include components for:\n",
    "\n",
    "- Processing: Preprocessing data and evaluating models.\n",
    "- Training: Training a model. This includes defining the training script and setting up the training environment.\n",
    "- Model: Creating a model in Amazon SageMaker.\n",
    "- Deploy: Deploying a model to a real-time endpoint.\n",
    "\n",
    "For more information, you can check out the following resources:\n",
    "\n",
    "- [Amazon SageMaker Components for Kubeflow Pipelines on GitHub](https://github.com/kubeflow/pipelines/tree/master/components/aws/sagemaker)\n",
    "- [Using Amazon SageMaker with Kubeflow Pipelines](https://docs.aws.amazon.com/sagemaker/latest/dg/usingamazon-sagemaker-components.html)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this section of the code, we're using the `load_component_from_url` function from the `kfp.components` module to load the Amazon SageMaker components. These components are defined in YAML files hosted on GitHub. Each component corresponds to a specific operation in Amazon SageMaker, such as processing, training, creating a model, or deploying a model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'gaecontrib' from 'requests_toolbelt._compat' (/Users/ADAM-Production/StudioProjects/gpt_ebook_finetuning/venv/lib/python3.9/site-packages/requests_toolbelt/_compat.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkfp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkfp\u001b[39;00m \u001b[39mimport\u001b[39;00m components\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkfp\u001b[39;00m \u001b[39mimport\u001b[39;00m dsl\n",
      "File \u001b[0;32m~/StudioProjects/gpt_ebook_finetuning/venv/lib/python3.9/site-packages/kfp/__init__.py:16\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Copyright 2018 Google LLC\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_client\u001b[39;00m \u001b[39mimport\u001b[39;00m Client\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_runners\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/StudioProjects/gpt_ebook_finetuning/venv/lib/python3.9/site-packages/kfp/_client.py:33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkfp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompiler\u001b[39;00m \u001b[39mimport\u001b[39;00m compiler\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkfp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompiler\u001b[39;00m \u001b[39mimport\u001b[39;00m _k8s_helper\n\u001b[0;32m---> 33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkfp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_auth\u001b[39;00m \u001b[39mimport\u001b[39;00m get_auth_token, get_gcp_access_token\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_add_generated_apis\u001b[39m(target_struct, api_module, api_client):\n\u001b[1;32m     38\u001b[0m \u001b[39m  \u001b[39m\u001b[39m'''Initializes a hierarchical API object based on the generated API module.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m  PipelineServiceApi.create_pipeline becomes target_struct.pipelines.create_pipeline\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m  '''\u001b[39;00m\n",
      "File \u001b[0;32m~/StudioProjects/gpt_ebook_finetuning/venv/lib/python3.9/site-packages/kfp/_auth.py:24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moauth2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcredentials\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moauth2\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mservice_account\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests_toolbelt\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39madapters\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mappengine\u001b[39;00m\n\u001b[1;32m     26\u001b[0m IAM_SCOPE \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhttps://www.googleapis.com/auth/iam\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     27\u001b[0m OAUTH_TOKEN_URI \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhttps://www.googleapis.com/oauth2/v4/token\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[0;32m~/StudioProjects/gpt_ebook_finetuning/venv/lib/python3.9/site-packages/requests_toolbelt/adapters/appengine.py:42\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrequests\u001b[39;00m \u001b[39mimport\u001b[39;00m sessions\n\u001b[1;32m     41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m exceptions \u001b[39mas\u001b[39;00m exc\n\u001b[0;32m---> 42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_compat\u001b[39;00m \u001b[39mimport\u001b[39;00m gaecontrib\n\u001b[1;32m     43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_compat\u001b[39;00m \u001b[39mimport\u001b[39;00m timeout\n\u001b[1;32m     46\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mAppEngineMROHack\u001b[39;00m(adapters\u001b[39m.\u001b[39mHTTPAdapter):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'gaecontrib' from 'requests_toolbelt._compat' (/Users/ADAM-Production/StudioProjects/gpt_ebook_finetuning/venv/lib/python3.9/site-packages/requests_toolbelt/_compat.py)"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp import components\n",
    "from kfp import dsl\n",
    "from kfp.aws import use_aws_secret\n",
    "\n",
    "# Load the components\n",
    "sagemaker_process_op = components.load_component_from_url(\n",
    "    \"https://raw.githubusercontent.com/kubeflow/pipelines/3ebd075212e0a761b982880707ec497c36a99d80/components/aws/sagemaker/process/component.yaml\"\n",
    ")\n",
    "sagemaker_train_op = components.load_component_from_url(\n",
    "    \"https://raw.githubusercontent.com/kubeflow/pipelines/3ebd075212e0a761b982880707ec497c36a99d80/components/aws/sagemaker/train/component.yaml\"\n",
    ")\n",
    "sagemaker_model_op = components.load_component_from_url(\n",
    "    \"https://raw.githubusercontent.com/kubeflow/pipelines/3ebd075212e0a761b982880707ec497c36a99d80/components/aws/sagemaker/model/component.yaml\"\n",
    ")\n",
    "sagemaker_deploy_op = components.load_component_from_url(\n",
    "    \"https://raw.githubusercontent.com/kubeflow/pipelines/3ebd075212e0a761b982880707ec497c36a99d80/components/aws/sagemaker/deploy/component.yaml\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Pre-Processing Code\n",
    "\n",
    "The pre-processing code is responsible for transforming raw data into a format that the model can be trained on. In this case, we're using a script named `preprocess-scikit-text-to-bert-feature-store.py`. We'll upload this script to an S3 bucket so that it can be accessed during the processing stage of the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_code_s3_uri = \"s3://{}/processing_code/preprocess-scikit-text-to-bert-feature-store.py\".format(bucket)\n",
    "print(processing_code_s3_uri)\n",
    "\n",
    "!aws s3 cp ./preprocess-scikit-text-to-bert-feature-store.py $processing_code_s3_uri"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package and Upload Training Code to S3\n",
    "Next, we'll package and upload the training code to S3. The training code is responsible for defining and training the model. We'll package the code into a tarball and then upload it to an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -cvzf sourcedir.tar.gz -C ./code .\n",
    "training_code_s3_uri = \"s3://{}/training_code/sourcedir.tar.gz\".format(bucket)\n",
    "print(training_code_s3_uri)\n",
    "\n",
    "!aws s3 cp sourcedir.tar.gz $training_code_s3_uri"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions\n",
    "We'll define a few helper functions that will be used to configure the inputs and outputs for the processing and training stages of the pipeline.\n",
    "\n",
    "\n",
    "In the `processing_input` function, we're defining the configuration for the input data for the processing stage. This includes the name of the input, the S3 URI where the input data is stored, the local path where the input data will be downloaded to, and the data distribution type.\n",
    "\n",
    "The `processing_output` function is similar, but it's used to define the configuration for the output data from the processing stage.\n",
    "\n",
    "The `training_input` function is used to define the configuration for the input data for the training stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_input(input_name, s3_uri, local_path, s3_data_distribution_type):\n",
    "    return {\n",
    "        \"InputName\": input_name,\n",
    "        \"S3Input\": {\n",
    "            \"LocalPath\": local_path,\n",
    "            \"S3Uri\": s3_uri,\n",
    "            \"S3DataType\": \"S3Prefix\",\n",
    "            \"S3DataDistributionType\": s3_data_distribution_type,\n",
    "            \"S3InputMode\": \"File\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "def processing_output(output_name, s3_uri, local_path, s3_upload_mode):\n",
    "    return {\n",
    "        \"OutputName\": output_name,\n",
    "        \"S3Output\": {\"LocalPath\": local_path, \"S3Uri\": s3_uri, \"S3UploadMode\": s3_upload_mode},\n",
    "    }\n",
    "\n",
    "def training_input(input_name, s3_uri, s3_data_distribution_type):\n",
    "    return {\n",
    "        \"ChannelName\": input_name,\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"S3Uri\": s3_uri,\n",
    "                \"S3DataType\": \"S3Prefix\",\n",
    "                \"S3DataDistributionType\": s3_data_distribution_type,\n",
    "            }\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Pipeline\n",
    "\n",
    "The pipeline is defined using the `@dsl.pipeline` decorator, which takes in the name and description of the pipeline. The pipeline function `bert_pipeline` takes in the role, bucket, region, and raw_input_data_s3_uri as arguments.\n",
    "\n",
    "The pipeline consists of three main stages: feature engineering, training, and deployment.\n",
    "\n",
    "In the feature engineering stage, we're using the sagemaker_process_op to run the pre-processing script that we uploaded to S3 earlier. The output of this stage is the processed data, which is stored in S3.\n",
    "\n",
    "In the training stage, we're using the sagemaker_train_op to train the model. The training script and the processed data from the previous stage are used as inputs. The output of this stage is the trained model, which is stored in S3.\n",
    "\n",
    "In the deployment stage, we're using the sagemaker_model_op to create a model from the trained model artifact, and then the sagemaker_deploy_op to deploy the model to a SageMaker endpoint.\n",
    "\n",
    "Each stage in the pipeline is connected using the .after() method, which ensures that the stages are executed in the correct order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "    name=\"BERT Pipeline\",  # Name of the pipeline\n",
    "    description=\"BERT Pipeline\",  # Description of the pipeline\n",
    ")\n",
    "def bert_pipeline(role=role, bucket=bucket, region=region, raw_input_data_s3_uri=raw_input_data_s3_uri):\n",
    "    # Import necessary libraries\n",
    "    import time\n",
    "    import json\n",
    "\n",
    "    # Define the pipeline name with a unique timestamp\n",
    "    pipeline_name = \"kubeflow-pipeline-sagemaker-{}\".format(int(time.time()))\n",
    "\n",
    "    # Set network isolation to False\n",
    "    network_isolation = False\n",
    "\n",
    "    ########################\n",
    "    # FEATURE ENGINEERING\n",
    "    ########################\n",
    "\n",
    "    # Define parameters for feature engineering\n",
    "    max_seq_length = 64\n",
    "    train_split_percentage = 0.90\n",
    "    validation_split_percentage = 0.05\n",
    "    test_split_percentage = 0.05\n",
    "    balance_dataset = True\n",
    "\n",
    "    # Define S3 URIs for the processed data\n",
    "    processed_train_data_s3_uri = \"s3://{}/{}/processing/output/bert-train\".format(bucket, pipeline_name)\n",
    "    processed_validation_data_s3_uri = \"s3://{}/{}/processing/output/bert-validation\".format(bucket, pipeline_name)\n",
    "    processed_test_data_s3_uri = \"s3://{}/{}/processing/output/bert-test\".format(bucket, pipeline_name)\n",
    "\n",
    "    # Define instance type and count for processing\n",
    "    processing_instance_type = \"ml.c5.2xlarge\"\n",
    "    processing_instance_count = 2\n",
    "\n",
    "    # Define feature store prefix and group name\n",
    "    timestamp = int(time.time())\n",
    "    feature_store_offline_prefix = \"reviews-feature-store-\" + str(timestamp)\n",
    "    feature_group_name = \"reviews-feature-group-\" + str(timestamp)\n",
    "\n",
    "    # Define the processing image\n",
    "    processing_image = \"683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3\"\n",
    "\n",
    "    # Define the processing operation\n",
    "   \n",
    "    process = sagemaker_process_op(\n",
    "        role=role,\n",
    "        region=region,\n",
    "        image=processing_image,\n",
    "        network_isolation=network_isolation,\n",
    "        instance_type=processing_instance_type,\n",
    "        instance_count=processing_instance_count,\n",
    "        container_arguments=[\n",
    "            \"--train-split-percentage\",\n",
    "            str(train_split_percentage),\n",
    "            \"--validation-split-percentage\",\n",
    "            str(validation_split_percentage),\n",
    "            \"--test-split-percentage\",\n",
    "            str(test_split_percentage),\n",
    "            \"--max-seq-length\",\n",
    "            str(max_seq_length),\n",
    "            \"--balance-dataset\",\n",
    "            str(balance_dataset),\n",
    "            \"--feature-store-offline-prefix\",\n",
    "            str(feature_store_offline_prefix),\n",
    "            \"--feature-group-name\",\n",
    "            str(feature_group_name),\n",
    "        ],\n",
    "        environment={\"AWS_DEFAULT_REGION\": \"eu-central-1\"},  # hard-coding to avoid serialization issue\n",
    "        container_entrypoint=[\n",
    "            \"python3\",\n",
    "            \"/opt/ml/processing/input/code/preprocess-scikit-text-to-bert-feature-store.py\",\n",
    "        ],\n",
    "        input_config=[\n",
    "            processing_input(\n",
    "                input_name=\"raw-input-data\",\n",
    "                s3_uri=\"{}\".format(raw_input_data_s3_uri),\n",
    "                local_path=\"/opt/ml/processing/input/data/\",\n",
    "                s3_data_distribution_type=\"ShardedByS3Key\",\n",
    "            ),\n",
    "            processing_input(\n",
    "                input_name=\"code\",\n",
    "                s3_uri=\"{}\".format(processing_code_s3_uri),\n",
    "                local_path=\"/opt/ml/processing/input/code\",\n",
    "                s3_data_distribution_type=\"FullyReplicated\",\n",
    "            ),\n",
    "        ],\n",
    "        output_config=[\n",
    "            processing_output(\n",
    "                output_name=\"bert-train\",\n",
    "                s3_uri=\"{}\".format(processed_train_data_s3_uri),\n",
    "                local_path=\"/opt/ml/processing/output/bert/train\",\n",
    "                s3_upload_mode=\"EndOfJob\",\n",
    "            ),\n",
    "            processing_output(\n",
    "                output_name=\"bert-validation\",\n",
    "                s3_uri=\"{}\".format(processed_validation_data_s3_uri),\n",
    "                local_path=\"/opt/ml/processing/output/bert/validation\",\n",
    "                s3_upload_mode=\"EndOfJob\",\n",
    "            ),\n",
    "            processing_output(\n",
    "                output_name=\"bert-test\",\n",
    "                s3_uri=\"{}\".format(processed_test_data_s3_uri),\n",
    "                local_path=\"/opt/ml/processing/output/bert/test\",\n",
    "                s3_upload_mode=\"EndOfJob\",\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    ########################\n",
    "    # TRAIN\n",
    "    ########################\n",
    "\n",
    "    # Define the training channels\n",
    "    train_channels = [\n",
    "        training_input(\n",
    "            input_name=\"train\", s3_uri=processed_train_data_s3_uri, s3_data_distribution_type=\"ShardedByS3Key\"\n",
    "        ),\n",
    "        training_input(\n",
    "            input_name=\"validation\",\n",
    "            s3_uri=processed_validation_data_s3_uri,\n",
    "            s3_data_distribution_type=\"ShardedByS3Key\",\n",
    "        ),\n",
    "        training_input(\n",
    "            input_name=\"test\", s3_uri=processed_test_data_s3_uri, s3_data_distribution_type=\"ShardedByS3Key\"\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Define hyperparameters for training\n",
    "    epochs = 1\n",
    "    learning_rate = 0.00001\n",
    "    epsilon = 0.00000001\n",
    "    train_batch_size = 128\n",
    "    validation_batch_size = 128\n",
    "    test_batch_size = 128\n",
    "    train_steps_per_epoch = 100\n",
    "    validation_steps = 100\n",
    "    test_steps = 100\n",
    "    train_volume_size = 1024\n",
    "    use_xla = True\n",
    "    use_amp = True\n",
    "    freeze_bert_layer = False\n",
    "    enable_sagemaker_debugger = False\n",
    "    enable_checkpointing = False\n",
    "    enable_tensorboard = False\n",
    "    input_mode = \"File\"\n",
    "    run_validation = True\n",
    "    run_test = True\n",
    "    run_sample_predictions = True\n",
    "\n",
    "    train_instance_type = \"ml.c5.9xlarge\"\n",
    "    train_instance_count = 1\n",
    "\n",
    "    train_output_location = \"s3://{}/{}/output\".format(bucket, pipeline_name)\n",
    "\n",
    "    hyperparameters = {\n",
    "        \"epochs\": \"{}\".format(epochs),\n",
    "        \"learning_rate\": \"{}\".format(learning_rate),\n",
    "        \"epsilon\": \"{}\".format(epsilon),\n",
    "        \"train_batch_size\": \"{}\".format(train_batch_size),\n",
    "        \"validation_batch_size\": \"{}\".format(validation_batch_size),\n",
    "        \"test_batch_size\": \"{}\".format(test_batch_size),\n",
    "        \"train_steps_per_epoch\": \"{}\".format(train_steps_per_epoch),\n",
    "        \"validation_steps\": \"{}\".format(validation_steps),\n",
    "        \"test_steps\": \"{}\".format(test_steps),\n",
    "        \"use_xla\": \"{}\".format(use_xla),\n",
    "        \"use_amp\": \"{}\".format(use_amp),\n",
    "        \"max_seq_length\": \"{}\".format(max_seq_length),\n",
    "        \"freeze_bert_layer\": \"{}\".format(freeze_bert_layer),\n",
    "        \"enable_sagemaker_debugger\": \"{}\".format(enable_sagemaker_debugger),\n",
    "        \"enable_checkpointing\": \"{}\".format(enable_checkpointing),\n",
    "        \"enable_tensorboard\": \"{}\".format(enable_tensorboard),\n",
    "        \"run_validation\": \"{}\".format(run_validation),\n",
    "        \"run_test\": \"{}\".format(run_test),\n",
    "        \"run_sample_predictions\": \"{}\".format(run_sample_predictions),\n",
    "        \"model_dir\": \"{}\".format(train_output_location),\n",
    "        \"sagemaker_program\": \"tf_bert_reviews.py\",\n",
    "        \"sagemaker_region\": \"{}\".format(region),\n",
    "        \"sagemaker_submit_directory\": training_code_s3_uri,\n",
    "    }\n",
    "    hyperparameters_json = json.dumps(hyperparameters)\n",
    "\n",
    "    # metric_definitions='{\"val_acc\": \"val_accuracy: ([0-9\\\\\\\\.]+)\"}',\n",
    "    metrics_definitions = [\n",
    "        {\"Name\": \"train:loss\", \"Regex\": \"loss: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"train:accuracy\", \"Regex\": \"accuracy: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"validation:loss\", \"Regex\": \"val_loss: ([0-9\\\\.]+)\"},\n",
    "        {\"Name\": \"validation:accuracy\", \"Regex\": \"val_accuracy: ([0-9\\\\.]+)\"},\n",
    "    ]\n",
    "    metrics_definitions_json = json.dumps(metrics_definitions)\n",
    "    print(metrics_definitions_json)\n",
    "\n",
    "    # .after(process) is explicitly appended below\n",
    "    train_image = \"763104351884.dkr.ecr.{}.amazonaws.com/tensorflow-training:2.3.1-cpu-py37-ubuntu18.04\".format(region)\n",
    "    training = sagemaker_train_op(\n",
    "        region=region,\n",
    "        image=train_image,\n",
    "        network_isolation=network_isolation,\n",
    "        instance_type=train_instance_type,\n",
    "        instance_count=train_instance_count,\n",
    "        hyperparameters=hyperparameters_json,\n",
    "        training_input_mode=input_mode,\n",
    "        channels=train_channels,\n",
    "        model_artifact_path=train_output_location,\n",
    "        # metric_definitions=metrics_definitions_json,\n",
    "        # TODO:  Add rules\n",
    "        role=role,\n",
    "    ).after(process)\n",
    "    ########################\n",
    "    # DEPLOY\n",
    "    ########################\n",
    "\n",
    "    # Define the serving image\n",
    "    serve_image = \"763104351884.dkr.ecr.{}.amazonaws.com/tensorflow-inference:2.3.1-cpu\".format(region)\n",
    "\n",
    "    # Define the model creation operation\n",
    "    create_model = sagemaker_model_op(\n",
    "        region=region,\n",
    "        model_name=training.outputs[\"job_name\"],\n",
    "        image=serve_image,\n",
    "        network_isolation=network_isolation,\n",
    "        model_artifact_url=training.outputs[\"model_artifact_url\"],\n",
    "        role=role,\n",
    "    )\n",
    "\n",
    "    # Define the deployment operation\n",
    "    deploy_instance_type = \"ml.c5.9xlarge\"\n",
    "    deploy_instance_count = 1\n",
    "    # .after(create_model) is implied because we depend on create_model.outputs\n",
    "    deploy_model = sagemaker_deploy_op(\n",
    "        region=region,\n",
    "        variant_name_1=\"AllTraffic\",\n",
    "        model_name_1=create_model.output,\n",
    "        instance_type_1=deploy_instance_type,\n",
    "        initial_instance_count_1=deploy_instance_count,\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Kubeflow Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kfp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Compile the Kubeflow pipeline\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m kfp\u001b[39m.\u001b[39mcompiler\u001b[39m.\u001b[39mCompiler()\u001b[39m.\u001b[39mcompile(bert_pipeline, \u001b[39m\"\u001b[39m\u001b[39mbert-pipeline.zip\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39m# List the details of the compiled pipeline file\u001b[39;00m\n\u001b[1;32m      5\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mls -al ./bert-pipeline.zip\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kfp' is not defined"
     ]
    }
   ],
   "source": [
    "# Compile the Kubeflow pipeline\n",
    "kfp.compiler.Compiler().compile(bert_pipeline, \"bert-pipeline.zip\")\n",
    "\n",
    "# List the details of the compiled pipeline file\n",
    "!ls -al ./bert-pipeline.zip\n",
    "\n",
    "# Unzip the compiled pipeline file\n",
    "!unzip -o ./bert-pipeline.zip\n",
    "\n",
    "# Display the contents of the pipeline.yaml file using pygmentize for syntax highlighting\n",
    "!pygmentize pipeline.yaml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Pipeline on Kubernetes Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the pipeline on the Kubernetes cluster\n",
    "client = kfp.Client()\n",
    "\n",
    "# Create an experiment in Kubeflow\n",
    "experiment = client.create_experiment(name=\"kubeflow\")\n",
    "\n",
    "# Run the pipeline within the context of the experiment\n",
    "my_run = client.run_pipeline(experiment.id, \"bert-pipeline\", \"bert-pipeline.zip\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above training job may take 5-10 minutes. Please be patient.\n",
    "In the meantime, open the SageMaker Console to monitor the progress of your training job.\n",
    "\n",
    "First, we need to get the endpoint name of our newly-deployed SageMaker Prediction Endpoint.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code first compiles the defined Kubeflow pipeline into a zip file, then it unzips the file and displays the contents of the pipeline.yaml file. After that, it creates a new experiment in Kubeflow and runs the pipeline within the context of the experiment.\n",
    "\n",
    "Once the pipeline is run, it waits for the training job to complete. After the training job is completed, it retrieves the endpoint name of the newly deployed SageMaker prediction endpoint. Then, it uses this endpoint to make predictions on some sample inputs and prints the predicted classes for each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import boto3 library for AWS operations\n",
    "import boto3\n",
    "\n",
    "# Create a SageMaker runtime client\n",
    "sm_runtime = boto3.Session(region_name=region).client(\"sagemaker-runtime\")\n",
    "\n",
    "# Replace this with the actual endpoint name from the Kubeflow pipeline logs\n",
    "endpoint_name = \"<COPY-AND-PASTE-FROM-KUBEFLOW-PIPELINE-LOGS>\"\n",
    "\n",
    "# Define the inputs for prediction\n",
    "inputs = [{\"features\": [\"This is great!\"]}, {\"features\": [\"This is bad.\"]}]\n",
    "\n",
    "# Invoke the SageMaker endpoint for prediction\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=\"application/jsonlines\",\n",
    "    Accept=\"application/jsonlines\",\n",
    "    Body=json.dumps(inputs).encode(\"utf-8\"),\n",
    ")\n",
    "\n",
    "# Print the response from the prediction\n",
    "print(\"response: {}\".format(response))\n",
    "\n",
    "# Parse the predicted classes from the response\n",
    "predicted_classes_str = response[\"Body\"].read().decode()\n",
    "predicted_classes_json = json.loads(predicted_classes_str)\n",
    "\n",
    "predicted_classes = predicted_classes_json.splitlines()\n",
    "print(\"predicted_classes: {}\".format(predicted_classes))\n",
    "\n",
    "# Print the predicted class for each input\n",
    "for predicted_class_json, input_data in zip(predicted_classes, inputs):\n",
    "    predicted_class = json.loads(predicted_class_json)[\"predicted_label\"]\n",
    "    print('Predicted star_rating: {} for review_body \"{}\"'.format(predicted_class, input_data[\"features\"][0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a68e2ec38d346fc0a81cce9ab4074dbd84a3a16f4775ed0b6a963af8256663df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
